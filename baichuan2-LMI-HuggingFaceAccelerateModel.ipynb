{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee041d9",
   "metadata": {},
   "source": [
    "# 在SageMaker上使用LMI,HuggingFaceAccelerateModel部署BaiChuan2模型\n",
    "\n",
    "syshen@amazon.com\n",
    "\n",
    "---------------\n",
    "repo_id=\"baichuan-inc/Baichuan2-7B-Base\"\n",
    "\n",
    "repo_id=\"baichuan-inc/Baichuan2-13B-Base\"\n",
    "\n",
    "repo_id=\"baichuan-inc/Baichuan2-7B-Chat\"\n",
    "\n",
    "repo_id=\"baichuan-inc/Baichuan2-7B-Chat-4bits\"\n",
    "\n",
    "repo_id=\"baichuan-inc/Baichuan2-13B-Chat\"\n",
    "\n",
    "repo_id=\"baichuan-inc/Baichuan2-13B-Chat-4bits\"\n",
    "\n",
    "时间:20~40分钟\n",
    "\n",
    "1.环境设置,下载、上传模型\n",
    "\n",
    "2.部署模型\n",
    "\n",
    "3.推理测试\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8182f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose repo_id\n",
    "repo_id=\"baichuan-inc/Baichuan2-7B-Base\"  #change this to yours\n",
    "model_name=repo_id.split(\"/\")[-1]\n",
    "local_dir=repo_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d2c72",
   "metadata": {},
   "source": [
    "## 1.环境设置,下载、上传模型 ~4分钟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254694d-3602-443a-b3c5-370a09d29cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#upgrade library\n",
    "!pip install -qU sagemaker\n",
    "!pip install -qU huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ed1be-985e-445c-9eee-e8fa1aecd71d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sagemaker environment setting\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "import shutil\n",
    "import sagemaker.huggingface\n",
    "\n",
    "from sagemaker.djl_inference.model import DJLModel,DeepSpeedModel,HuggingFaceAccelerateModel,DJLPredictor\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = sagemaker_session.boto_region_name\n",
    "account_id = sagemaker_session.account_id\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a21a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download s5cmd\n",
    "!curl -L https://github.com/peak/s5cmd/releases/download/v2.0.0/s5cmd_2.0.0_Linux-64bit.tar.gz | tar -xz\n",
    "!chmod 777 s5cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1002b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download BaiChuan2 model file from Hugging Face\n",
    "model_download_path = snapshot_download(repo_id=repo_id,local_dir=local_dir,ignore_patterns=[\"*.msgpack\",\"*.h5\"])\n",
    "print(model_download_path)\n",
    "!ls $local_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348fe048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload model files to s3 bucket\n",
    "!./s5cmd sync $local_dir/ s3://$bucket/$repo_id/\n",
    "!aws s3 ls s3://$bucket/$repo_id/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c28dbc-5a37-495c-9aab-eaf1a54935d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.部署模型 ~10分钟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76de19c-c3af-4887-a624-d6d43b115921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prepare inference files\n",
    "source_dir = 'source_dir'\n",
    "entry_point = f'{model_name}.py'\n",
    "\n",
    "if os.path.exists(source_dir):\n",
    "    shutil.rmtree(source_dir)\n",
    "!mkdir $source_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddeb489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy\n",
    "!cp $local_dir/*.py $source_dir\n",
    "!cp $local_dir/*.json $source_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d106056",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_dir/serving.properties\n",
    "engine=python\n",
    "option.enable_streaming = huggingface\n",
    "option.dtype = fp16\n",
    "option.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fce9c5-5e9c-43c5-9cd6-e03721038771",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_dir/requirements.txt\n",
    "numpy\n",
    "transformers==4.33.1 \n",
    "tokenizers\n",
    "sentencepiece \n",
    "bitsandbytes \n",
    "deepspeed>=0.8.3\n",
    "xformers \n",
    "accelerate\n",
    "vllm\n",
    "pandas\n",
    "scipy\n",
    "nvgpu\n",
    "pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9dac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_dir/Baichuan2-7B-Base.py\n",
    "#training code\n",
    "import torch\n",
    "from djl_python import Input, Output\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "from tokenization_baichuan import BaichuanTokenizer\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "import os\n",
    "\n",
    "print(f\"torch.__version__:{torch.__version__}\")\n",
    "print(f\"transformers.__version__:{transformers.__version__}\")\n",
    "cwd = os.getcwd()\n",
    "print(f\"cwd:{cwd}\")\n",
    "dir_list = os.listdir(cwd)\n",
    "print(\"Files and directories in '\", cwd, \"' :\")\n",
    "# prints all files\n",
    "print(dir_list)\n",
    "\n",
    "def load_model(properties):\n",
    "    global model,tokenizer\n",
    "    \n",
    "    model_name = properties[\"model_id\"]\n",
    "    \n",
    "    tensor_parallel_degree = int(properties[\"tensor_parallel_degree\"])\n",
    "    pipeline_parallel_degree = 1\n",
    "    dtype = properties[\"dtype\"]\n",
    "    \n",
    "    logging.info(f\"Loading model: {model_name}\")\n",
    "    print(f'model_name:{model_name}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16,trust_remote_code=True)\n",
    "    \n",
    "    return model,tokenizer\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model,tokenizer\n",
    "    max_new_tokens = 64\n",
    "    print('----inputs.get_properties():----')\n",
    "    print(inputs.get_properties())\n",
    "    \n",
    "    if not model:\n",
    "        print(\"----no model----starting load_model-----\")\n",
    "        model,tokenizer = load_model(inputs.get_properties())\n",
    "    \n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        print(\"----return None-----\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"----return {inputs}-----\")\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    data = inputs.get_as_json()\n",
    "    input_text = data[\"inputs\"]\n",
    "    params = data[\"parameters\"]\n",
    " \n",
    "    #messages = []\n",
    "    #messages.append({\"role\": \"user\", \"content\": input_text})\n",
    "    #response = model.chat(tokenizer, messages)\n",
    "    \n",
    "    inputs = tokenizer(input_text, return_tensors='pt')\n",
    "    inputs = inputs.to('cuda:0')\n",
    "    pred = model.generate(**inputs, max_new_tokens=max_new_tokens, repetition_penalty=1.1)\n",
    "    response = tokenizer.decode(pred.cpu()[0], skip_special_tokens=True)\n",
    "    print(response)\n",
    "\n",
    "    result = {\"BaiChuan2\":response}\n",
    "    \n",
    "    return Output().add(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc28e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_dir/Baichuan2-13B-Base.py\n",
    "#training code\n",
    "import torch\n",
    "from djl_python import Input, Output\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "from tokenization_baichuan import BaichuanTokenizer\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "import os\n",
    "\n",
    "print(f\"torch.__version__:{torch.__version__}\")\n",
    "print(f\"transformers.__version__:{transformers.__version__}\")\n",
    "cwd = os.getcwd()\n",
    "print(f\"cwd:{cwd}\")\n",
    "dir_list = os.listdir(cwd)\n",
    "print(\"Files and directories in '\", cwd, \"' :\")\n",
    "# prints all files\n",
    "print(dir_list)\n",
    "\n",
    "def load_model(properties):\n",
    "    global model,tokenizer\n",
    "    \n",
    "    model_name = properties[\"model_id\"]\n",
    "    \n",
    "    tensor_parallel_degree = int(properties[\"tensor_parallel_degree\"])\n",
    "    pipeline_parallel_degree = 1\n",
    "    dtype = properties[\"dtype\"]\n",
    "    \n",
    "    logging.info(f\"Loading model: {model_name}\")\n",
    "    print(f'model_name:{model_name}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16,trust_remote_code=True)\n",
    "    \n",
    "    return model,tokenizer\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model,tokenizer\n",
    "    max_new_tokens = 64\n",
    "    print('----inputs.get_properties():----')\n",
    "    print(inputs.get_properties())\n",
    "    \n",
    "    if not model:\n",
    "        print(\"----no model----starting load_model-----\")\n",
    "        model,tokenizer = load_model(inputs.get_properties())\n",
    "    \n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        print(\"----return None-----\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"----return {inputs}-----\")\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    data = inputs.get_as_json()\n",
    "    input_text = data[\"inputs\"]\n",
    "    params = data[\"parameters\"]\n",
    " \n",
    "    #messages = []\n",
    "    #messages.append({\"role\": \"user\", \"content\": input_text})\n",
    "    #response = model.chat(tokenizer, messages)\n",
    "    \n",
    "    inputs = tokenizer(input_text, return_tensors='pt')\n",
    "    inputs = inputs.to('cuda:0')\n",
    "    pred = model.generate(**inputs, max_new_tokens=max_new_tokens, repetition_penalty=1.1)\n",
    "    response = tokenizer.decode(pred.cpu()[0], skip_special_tokens=True)\n",
    "    print(response)\n",
    "\n",
    "    result = {\"BaiChuan2\":response}\n",
    "    \n",
    "    return Output().add(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca592306",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_dir/Baichuan2-7B-Chat.py\n",
    "#training code\n",
    "import torch\n",
    "from djl_python import Input, Output\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "from tokenization_baichuan import BaichuanTokenizer\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "import os\n",
    "\n",
    "print(f\"torch.__version__:{torch.__version__}\")\n",
    "print(f\"transformers.__version__:{transformers.__version__}\")\n",
    "cwd = os.getcwd()\n",
    "print(f\"cwd:{cwd}\")\n",
    "dir_list = os.listdir(cwd)\n",
    "print(\"Files and directories in '\", cwd, \"' :\")\n",
    "# prints all files\n",
    "print(dir_list)\n",
    "\n",
    "def load_model(properties):\n",
    "    model_name = properties[\"model_id\"]\n",
    "    \n",
    "    tensor_parallel_degree = int(properties[\"tensor_parallel_degree\"])\n",
    "    pipeline_parallel_degree = 1\n",
    "    dtype = properties[\"dtype\"]\n",
    "    \n",
    "    logging.info(f\"Loading model: {model_name}\")\n",
    "    print(f'model_name:{model_name}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False,trust_remote_code=True) #trust_remote_code=True\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16,trust_remote_code=True) #torch.bfloat16 trust_remote_code=True\n",
    "    model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "\n",
    "    return model,tokenizer\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model,tokenizer\n",
    "    \n",
    "    print('----inputs.get_properties():----')\n",
    "    print(inputs.get_properties())\n",
    "    \n",
    "    if not model:\n",
    "        print(\"----no model----starting load_model-----\")\n",
    "        model,tokenizer = load_model(inputs.get_properties())\n",
    "    \n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        print(\"----return None-----\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"----return {inputs}-----\")\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    data = inputs.get_as_json()\n",
    "    input_text = data[\"inputs\"]\n",
    "    params = data[\"parameters\"]\n",
    " \n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": input_text})\n",
    "    response = model.chat(tokenizer, messages)\n",
    "\n",
    "    result = {\"BaiChuan2\":response}\n",
    "    \n",
    "    return Output().add(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15445ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_dir/Baichuan2-7B-Chat-4bits.py\n",
    "#training code\n",
    "import torch\n",
    "from djl_python import Input, Output\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "from tokenization_baichuan import BaichuanTokenizer\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "import os\n",
    "\n",
    "print(f\"torch.__version__:{torch.__version__}\")\n",
    "print(f\"transformers.__version__:{transformers.__version__}\")\n",
    "cwd = os.getcwd()\n",
    "print(f\"cwd:{cwd}\")\n",
    "dir_list = os.listdir(cwd)\n",
    "print(\"Files and directories in '\", cwd, \"' :\")\n",
    "# prints all files\n",
    "print(dir_list)\n",
    "\n",
    "def load_model(properties):\n",
    "    model_name = properties[\"model_id\"]\n",
    "    \n",
    "    tensor_parallel_degree = int(properties[\"tensor_parallel_degree\"])\n",
    "    pipeline_parallel_degree = 1\n",
    "    dtype = properties[\"dtype\"]\n",
    "    \n",
    "    logging.info(f\"Loading model: {model_name}\")\n",
    "    print(f'model_name:{model_name}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False,trust_remote_code=True) #trust_remote_code=True\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16,trust_remote_code=True) #torch.bfloat16 trust_remote_code=True\n",
    "    model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "\n",
    "    return model,tokenizer\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model,tokenizer\n",
    "    \n",
    "    print('----inputs.get_properties():----')\n",
    "    print(inputs.get_properties())\n",
    "    \n",
    "    if not model:\n",
    "        print(\"----no model----starting load_model-----\")\n",
    "        model,tokenizer = load_model(inputs.get_properties())\n",
    "    \n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        print(\"----return None-----\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"----return {inputs}-----\")\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    data = inputs.get_as_json()\n",
    "    input_text = data[\"inputs\"]\n",
    "    params = data[\"parameters\"]\n",
    " \n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": input_text})\n",
    "    response = model.chat(tokenizer, messages)\n",
    "\n",
    "    result = {\"BaiChuan2\":response}\n",
    "    \n",
    "    return Output().add(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24cc19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_dir/Baichuan2-13B-Chat.py\n",
    "#training code\n",
    "import torch\n",
    "from djl_python import Input, Output\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "from tokenization_baichuan import BaichuanTokenizer\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "import os\n",
    "\n",
    "print(f\"torch.__version__:{torch.__version__}\")\n",
    "print(f\"transformers.__version__:{transformers.__version__}\")\n",
    "cwd = os.getcwd()\n",
    "print(f\"cwd:{cwd}\")\n",
    "dir_list = os.listdir(cwd)\n",
    "print(\"Files and directories in '\", cwd, \"' :\")\n",
    "# prints all files\n",
    "print(dir_list)\n",
    "\n",
    "def load_model(properties):\n",
    "    model_name = properties[\"model_id\"]\n",
    "    \n",
    "    tensor_parallel_degree = int(properties[\"tensor_parallel_degree\"])\n",
    "    pipeline_parallel_degree = 1\n",
    "    dtype = properties[\"dtype\"]\n",
    "    \n",
    "    logging.info(f\"Loading model: {model_name}\")\n",
    "    print(f'model_name:{model_name}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False,trust_remote_code=True) #trust_remote_code=True\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16,trust_remote_code=True) #torch.bfloat16 trust_remote_code=True\n",
    "    model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "\n",
    "    return model,tokenizer\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model,tokenizer\n",
    "    \n",
    "    print('----inputs.get_properties():----')\n",
    "    print(inputs.get_properties())\n",
    "    \n",
    "    if not model:\n",
    "        print(\"----no model----starting load_model-----\")\n",
    "        model,tokenizer = load_model(inputs.get_properties())\n",
    "    \n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        print(\"----return None-----\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"----return {inputs}-----\")\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    data = inputs.get_as_json()\n",
    "    input_text = data[\"inputs\"]\n",
    "    params = data[\"parameters\"]\n",
    " \n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": input_text})\n",
    "    response = model.chat(tokenizer, messages)\n",
    "\n",
    "    result = {\"BaiChuan2\":response}\n",
    "    \n",
    "    return Output().add(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_dir/Baichuan2-13B-Chat-4bits.py\n",
    "#training code\n",
    "import torch\n",
    "from djl_python import Input, Output\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "from tokenization_baichuan import BaichuanTokenizer\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "import os\n",
    "\n",
    "print(f\"torch.__version__:{torch.__version__}\")\n",
    "print(f\"transformers.__version__:{transformers.__version__}\")\n",
    "cwd = os.getcwd()\n",
    "print(f\"cwd:{cwd}\")\n",
    "dir_list = os.listdir(cwd)\n",
    "print(\"Files and directories in '\", cwd, \"' :\")\n",
    "# prints all files\n",
    "print(dir_list)\n",
    "\n",
    "def load_model(properties):\n",
    "    model_name = properties[\"model_id\"]\n",
    "    \n",
    "    tensor_parallel_degree = int(properties[\"tensor_parallel_degree\"])\n",
    "    pipeline_parallel_degree = 1\n",
    "    dtype = properties[\"dtype\"]\n",
    "    \n",
    "    logging.info(f\"Loading model: {model_name}\")\n",
    "    print(f'model_name:{model_name}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False,trust_remote_code=True) #trust_remote_code=True\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16,trust_remote_code=True) #torch.bfloat16 trust_remote_code=True\n",
    "    model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "\n",
    "    return model,tokenizer\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model,tokenizer\n",
    "    \n",
    "    print('----inputs.get_properties():----')\n",
    "    print(inputs.get_properties())\n",
    "    \n",
    "    if not model:\n",
    "        print(\"----no model----starting load_model-----\")\n",
    "        model,tokenizer = load_model(inputs.get_properties())\n",
    "    \n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        print(\"----return None-----\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"----return {inputs}-----\")\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    data = inputs.get_as_json()\n",
    "    input_text = data[\"inputs\"]\n",
    "    params = data[\"parameters\"]\n",
    " \n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": input_text})\n",
    "    response = model.chat(tokenizer, messages)\n",
    "\n",
    "    result = {\"BaiChuan2\":response}\n",
    "    \n",
    "    return Output().add(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23d2395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploying model\n",
    "import time\n",
    "\n",
    "endpoint_name = f\"{repo_id.replace('/', '-')}-HFModel-\" + time.strftime(\"%Y%m%d-%H%M%S\", time.gmtime())\n",
    "print(endpoint_name)\n",
    "model_id = f\"s3://{bucket}/{repo_id}/\"\n",
    "\n",
    "if(repo_id.endswith(\"-7B-Base\") or repo_id.endswith(\"-7B-Chat\") or repo_id.endswith(\"-Chat-4bits\")):\n",
    "    instance_type = \"ml.g5.2xlarge\"\n",
    "    number_of_partitions = 1\n",
    "elif(repo_id.endswith(\"-13B-Base\") or repo_id.endswith(\"-13B-Chat\")):\n",
    "    instance_type = \"ml.g5.12xlarge\"\n",
    "    number_of_partitions = 4\n",
    "else:\n",
    "    instance_type = \"ml.g5.2xlarge\"\n",
    "    number_of_partitions = 1\n",
    "print(f\"instance_type : {instance_type}\")\n",
    "\n",
    "model = HuggingFaceAccelerateModel(\n",
    "    model_id, # This can also be a HuggingFace Hub model id\n",
    "    role,\n",
    "    dtype=\"bf16\",\n",
    "    task=\"text-generation\",\n",
    "    number_of_partitions=number_of_partitions, # number of gpus to partition the model across\n",
    "    entry_point = entry_point,\n",
    "    source_dir = source_dir,\n",
    "    #load_in_8bit=True\n",
    ")\n",
    "   \n",
    "predictor = model.deploy(instance_type=instance_type,\n",
    "                         initial_instance_count=1,\n",
    "                         endpoint_name=endpoint_name,\n",
    "                         model_data_download_timeout=5*60,\n",
    "                         container_startup_health_check_timeout=8*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34ba532",
   "metadata": {},
   "source": [
    "## 3.推理测试~ 1分钟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "print(f'endpoint_name : {predictor.endpoint_name}')\n",
    "print(predictor.predict(\n",
    "    { \n",
    "        \"inputs\" : \"写一篇关于气候变化对海洋生态的影响的文章,\", \n",
    "        \"parameters\": { \"max_length\": 500 },\n",
    "    }\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ac099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40ffdb84-8f17-43ca-b5fe-348d01a27de8",
   "metadata": {},
   "source": [
    "## only for re-invoke already-created endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab252a6b-e7a4-4253-8281-85deff803b2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#only for re-invoke already-created endpoint\n",
    "endpoint_name = \"baichuan-inc--Baichuan2-7B-Base-2023-10-01-05-27-31\"\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.djl_inference.model import DJLModel,DeepSpeedModel,HuggingFaceAccelerateModel,DJLPredictor\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "#endpoint_name = \"djl-inference-2023-09-27-13-21-39-526\"\n",
    "predictor = DJLPredictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")\n",
    "#predict\n",
    "print(predictor.predict(\n",
    "    { \n",
    "        \"inputs\" : \"写一篇中秋放假游玩的散文,500字\", \n",
    "        \"parameters\": { \"max_length\": 50 },\n",
    "    }\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0176edd-0a56-4756-ba05-698db2464447",
   "metadata": {},
   "source": [
    "## clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85351ea4-e0e0-4194-b07c-605f243abd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "endpoint_name = \"djl-inference-2023-09-27-13-21-39-526\"\n",
    "model_name = \"\"\n",
    "sagemaker_session.delete_endpoint(endpoint_name)\n",
    "#sagemaker_session.delete_endpoint_config(endpoint_name)\n",
    "#sagemaker_session.delete_model(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fdb52f",
   "metadata": {},
   "source": [
    "# backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef21ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "framework=\"djl-deepspeed\",\n",
    "region='us-east-1',\n",
    "version=\"0.23.0\"\n",
    ")\n",
    "inference_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0aa4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id=\"baichuan-inc/Baichuan2-13B-Base\"\n",
    "if(repo_id.endswith(\"-7B-Base\") or repo_id.endswith(\"-7B-Chat\")):\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927656bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamScanner:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the InvokeEndpointWithResponseStream event stream.\n",
    "\n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "\n",
    "    While usually each PayloadPart event from the event stream will contain a byte array\n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "\n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'readlines' function. It maintains the position of the last read\n",
    "    position to ensure that previous bytes are not exposed again.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.buff = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def write(self, content: bytes) -> None:\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "\n",
    "    def readlines(self) -> bytes:\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            if line[-1] != b\"\\n\":\n",
    "                self.read_pos += len(line)\n",
    "                yield line[:-1]\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.read_pos = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "import io\n",
    "endpoint_name = \"baichuan-inc-Baichuan2-7B-Base-HFModel-20231010-071324\"\n",
    "prompts = \"What is Amazon? Be concise.\"\n",
    "request_content_type = \"application/json\"\n",
    "response_content_type = \"application/json\"\n",
    "SAGEMAKER_RUNTIME_CLIENT = boto3.client(\"sagemaker-runtime\", region_name=region)\n",
    "\n",
    "request_body = {\n",
    "    \"inputs\": prompts,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 1.1,\n",
    "        \"top_p\": 0.85,\n",
    "    },\n",
    "}\n",
    "\n",
    "response = SAGEMAKER_RUNTIME_CLIENT.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(request_body),\n",
    "    ContentType=request_content_type,\n",
    "    Accept=response_content_type,\n",
    ")\n",
    "\n",
    "event_stream = response[\"Body\"]\n",
    "scanner = StreamScanner()\n",
    "for event in event_stream:\n",
    "    scanner.write(event[\"PayloadPart\"][\"Bytes\"])\n",
    "    for line in scanner.readlines():\n",
    "        deserialized_line = json.loads(line)\n",
    "        print(deserialized_line.get(\"outputs\")[0], end=\"\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e2b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
