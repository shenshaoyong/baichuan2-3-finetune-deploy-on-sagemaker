{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b86afb1",
   "metadata": {},
   "source": [
    "# FineTune Baichuan2 deepspeed on SageMaker\n",
    "syshen@amazon.com\n",
    "\n",
    "repo_id=\"baichuan-inc/Baichuan2-7B-Base\"\n",
    "\n",
    "repo_id=\"baichuan-inc/Baichuan2-13B-Base\"\n",
    "\n",
    "时间:20~40分钟\n",
    "\n",
    "1.环境设置\n",
    "\n",
    "2.调优模型\n",
    "\n",
    "3.推理测试\n",
    "\n",
    "---------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047b5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id=\"baichuan-inc/Baichuan2-7B-Base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9332e77",
   "metadata": {},
   "source": [
    "## 1.环境设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU transformers datasets[s3] sagemaker \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd40f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker environment setting\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "import shutil\n",
    "import sagemaker.huggingface\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = sagemaker_session.boto_region_name\n",
    "account_id = sagemaker_session.account_id\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare training files\n",
    "source_dir = 'source_dir'\n",
    "entry_point = 'ds_launcher.py'\n",
    "\n",
    "if os.path.exists(source_dir):\n",
    "    shutil.rmtree(source_dir)\n",
    "!mkdir $source_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9025859",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_dir/requirements.txt\n",
    "numpy\n",
    "transformers==4.28.1\n",
    "sentencepiece\n",
    "tokenizers\n",
    "accelerate>=0.23.0\n",
    "deepspeed>=0.8.3\n",
    "xformers\n",
    "scipy\n",
    "bitsandbytes\n",
    "peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e3b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_dir/$entry_point\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = ArgumentParser(\n",
    "        description=(\"SageMaker DeepSpeed Launch helper utility that will spawn deepspeed training scripts\")\n",
    "    )\n",
    "    # positional\n",
    "    parser.add_argument(\n",
    "        \"--training_script\",\n",
    "        type=str,\n",
    "        help=\"Path to the training program/script to be run in parallel, can be either absolute or relative\",\n",
    "    )\n",
    "\n",
    "    # rest from the training program\n",
    "    parsed, nargs = parser.parse_known_args()\n",
    "\n",
    "    return parsed.training_script, nargs\n",
    "\n",
    "\n",
    "def main():\n",
    "    # https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/launcher/launch.py\n",
    "    num_gpus = int(os.environ.get(\"SM_NUM_GPUS\", 0))\n",
    "    hosts = json.loads(os.environ.get(\"SM_HOSTS\", \"{}\"))\n",
    "    num_nodes = len(hosts)\n",
    "    current_host = os.environ.get(\"SM_CURRENT_HOST\", 0)\n",
    "    rank = hosts.index(current_host)\n",
    "    print(f\"num_gpus = {num_gpus}, num_nodes = {num_nodes}, current_host = {current_host}, rank = {rank}\")\n",
    "\n",
    "    # os.environ['NCCL_DEBUG'] = 'INFO'\n",
    "\n",
    "    # get number of GPU\n",
    "    # if num_gpus == 0:\n",
    "    #     raise ValueError(\"No GPUs found.\")\n",
    "\n",
    "    train_script, args = parse_args()\n",
    "    #--hostfile=hostfile \n",
    "    command = f\"deepspeed --num_gpus={num_gpus} {train_script} {' '.join(args)}\"\n",
    "    print(f\"command = {command}\")\n",
    "    # launch deepspeed training\n",
    "    deepspeed_launch(command)\n",
    "\n",
    "\n",
    "def deepspeed_launch(command):\n",
    "    # try:\n",
    "    try:\n",
    "        subprocess.run(command, shell=True)\n",
    "    except Exception as e:\n",
    "        logger.info(e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed909f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_dir/fine-tune.py\n",
    "import os\n",
    "import math\n",
    "import pathlib\n",
    "from typing import Optional, Dict\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"baichuan-inc/Baichuan2-7B-Base\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"Path to the training data.\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "    use_lora: bool = field(default=False)\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        tokenizer,\n",
    "        model_max_length,\n",
    "        user_tokens=[195],\n",
    "        assistant_tokens=[196],\n",
    "    ):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        self.data = json.load(open(data_path))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_max_length = model_max_length\n",
    "        self.user_tokens = user_tokens\n",
    "        self.assistant_tokens = assistant_tokens\n",
    "        self.ignore_index = -100\n",
    "        item = self.preprocessing(self.data[0])\n",
    "        print(\"input:\", self.tokenizer.decode(item[\"input_ids\"]))\n",
    "        labels = []\n",
    "        for id_ in item[\"labels\"]:\n",
    "            if id_ == -100:\n",
    "                continue\n",
    "\n",
    "            labels.append(id_)\n",
    "        print(\"label:\", self.tokenizer.decode(labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def preprocessing(self, example):\n",
    "        input_ids = []\n",
    "        labels = []\n",
    "\n",
    "        for message in example[\"conversations\"]:\n",
    "            from_ = message[\"from\"]\n",
    "            value = message[\"value\"]\n",
    "            value_ids = self.tokenizer.encode(value)\n",
    "\n",
    "            if from_ == \"human\":\n",
    "                input_ids += self.user_tokens + value_ids\n",
    "                labels += [self.tokenizer.eos_token_id] + [self.ignore_index] * len(\n",
    "                    value_ids\n",
    "                )\n",
    "            else:\n",
    "                input_ids += self.assistant_tokens + value_ids\n",
    "                labels += [self.ignore_index] + value_ids\n",
    "        input_ids.append(self.tokenizer.eos_token_id)\n",
    "        labels.append(self.tokenizer.eos_token_id)\n",
    "        input_ids = input_ids[: self.model_max_length]\n",
    "        labels = labels[: self.model_max_length]\n",
    "        input_ids += [self.tokenizer.pad_token_id] * (\n",
    "            self.model_max_length - len(input_ids)\n",
    "        )\n",
    "        labels += [self.ignore_index] * (self.model_max_length - len(labels))\n",
    "        input_ids = torch.LongTensor(input_ids)\n",
    "        labels = torch.LongTensor(labels)\n",
    "        attention_mask = input_ids.ne(self.tokenizer.pad_token_id)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
    "        return self.preprocessing(self.data[idx])\n",
    "\n",
    "\n",
    "def train():\n",
    "    parser = transformers.HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, TrainingArguments)\n",
    "    )\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "    )\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        use_fast=False,\n",
    "        trust_remote_code=True,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "    )\n",
    "    if training_args.use_lora:\n",
    "        from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            target_modules=[\"W_pack\"],\n",
    "            inference_mode=False,\n",
    "            r=1,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "        )\n",
    "        model.enable_input_require_grads()\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    dataset = SupervisedDataset(\n",
    "        data_args.data_path, tokenizer, training_args.model_max_length\n",
    "    )\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model, args=training_args, train_dataset=dataset, tokenizer=tokenizer\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_state()\n",
    "    trainer.save_model(output_dir=training_args.output_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb473f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_dir/ds_config.json\n",
    "{\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\" :\"auto\",\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"bf16\": {\n",
    "        \"enabled\": \"auto\"\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"overlap_comm\": true,\n",
    "    \"stage3_gather_16bit_weights_on_model_save\": true\n",
    "    },\n",
    "    \"flops_profiler\": {\n",
    "        \"enabled\": false,\n",
    "        \"profile_step\": 1,\n",
    "        \"module_depth\": -1,\n",
    "        \"top_modules\": 1,\n",
    "        \"detailed\": true,\n",
    "        \"output_file\": null\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9995edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile $source_dir/hostfile\n",
    "# algo-1 slots=4\n",
    "# algo-2 slots=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6868d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting data_path\n",
    "!aws s3 cp data/belle_chat_ramdon_10k.json s3://$bucket/datasets-baichuan2/belle_chat_ramdon_10k.json\n",
    "\n",
    "data_path_s3 = f\"s3://{bucket}/datasets-baichuan2/\"\n",
    "print(f\"data_path_s3 : {data_path_s3}\")\n",
    "data_path = \"/opt/ml/input/data/train/belle_chat_ramdon_10k.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "deepspeed_parameters = {\n",
    "    \"deepspeed\": \"ds_config.json\", # deepspeed config file\n",
    "    \"training_script\": \"fine-tune.py\", # real training script, not entrypoint\n",
    "    \"report_to\": \"none\",\n",
    "    \"data_path\": data_path,#\"belle_chat_ramdon_10k.json\",\n",
    "    \"model_name_or_path\": repo_id, #\"baichuan-inc/Baichuan2-7B-Base\",\n",
    "    \"output_dir\": \"/opt/ml/model\",\n",
    "    \"model_max_length\": 512,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"lr_scheduler_type\": \"constant\",\n",
    "    \"adam_beta1\": 0.9,\n",
    "    \"adam_beta2\": 0.98,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"warmup_ratio\": 0.0,\n",
    "    \"logging_steps\": 1,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    #\"deepspeed\": \"ds_config.json\",\n",
    "    \"bf16\": True,\n",
    "    \"tf32\": True,\n",
    "    \"use_lora\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b256fb7",
   "metadata": {},
   "source": [
    "## 调优模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f2ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# define Training Job Name \n",
    "job_name = f'baichuan2-hf-deepspeed-{time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = entry_point,  # deepspeed launcher script\n",
    "    source_dir           = source_dir,               # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.p4d.24xlarge', # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28.1',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0.0',            # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',            # the python version used in the training job\n",
    "    hyperparameters      = {\n",
    "      **deepspeed_parameters\n",
    "    },   # the hyperparameter used for running the training job\n",
    "    disable_output_compression = True,\n",
    "#     checkpoint_s3_uri=f's3://{bucket}/baichuan2checkpoints',\n",
    "#     use_spot_instances=True,\n",
    "#     max_wait=36000,\n",
    "#     max_run=7200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba93db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "# data = {\n",
    "#     'training': training_input_path,\n",
    "#     'test': test_input_path\n",
    "# }\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "from sagemaker.inputs import TrainingInput\n",
    "train_input=TrainingInput(\n",
    "        s3_data=data_path_s3,\n",
    "        input_mode='File'  # Available options: File | Pipe | FastFile\n",
    "    )\n",
    "\n",
    "#start training\n",
    "huggingface_estimator.fit({\"train\": train_input},wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d93e9",
   "metadata": {},
   "source": [
    "## 合并lora和base模型\n",
    "\n",
    "注意:下面操作需要在带有GPU显卡的SageMaker Notebook instance或EC2(Deep Learning AMI GPU PyTorch 2.0.1 (Amazon Linux 2) 20231003 AMI名称)上操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b637cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#需要git clone baichuan2 github上的文件\n",
    "!git clone https://github.com/baichuan-inc/Baichuan2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc483279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#登录到机器上, 需要使用Torch2.0,安装下列requirements.txt内容\n",
    "numpy\n",
    "transformers==4.28.1\n",
    "sentencepiece\n",
    "tokenizers\n",
    "accelerate>=0.23.0\n",
    "deepspeed>=0.8.3\n",
    "xformers\n",
    "scipy\n",
    "bitsandbytes\n",
    "peft\n",
    "\n",
    "#进入到相应目录\n",
    "#./lora目录存放finetune训练后的模型,从上面训练好后的S3文件夹中下载下来 \n",
    "#pytorch_model.bin改名为adapter_model.bin; \n",
    "#添加adapter_config.json文件,内容如下\n",
    "{\n",
    "  \"base_model_name_or_path\": \"baichuan-inc/Baichuan2-7B-Base\",\n",
    "  \"bias\": \"none\",\n",
    "  \"enable_lora\": null,\n",
    "  \"fan_in_fan_out\": false,\n",
    "  \"inference_mode\": true,\n",
    "  \"lora_alpha\": 32,\n",
    "  \"lora_dropout\": 0.1,\n",
    "  \"merge_weights\": false,\n",
    "  \"modules_to_save\": null,\n",
    "  \"peft_type\": \"LORA\",\n",
    "  \"r\": 1,\n",
    "  \"target_modules\": [\n",
    "    \"W_pack\"\n",
    "  ],\n",
    "  \"task_type\": \"CAUSAL_LM\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54bca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./base目录存放base模型 可从huggingface或已存放的s3桶中下载\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf920ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建merge.py文件,内容如下\n",
    "from peft import PeftModel, LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"base\", local_files_only=True,trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"base\", local_files_only=True,device_map=\"auto\",torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "#inputs = tokenizer('登鹳雀楼->王之涣\\n夜雨寄北->', return_tensors='pt')\n",
    "#inputs = inputs.to('cuda:0')\n",
    "#pred = model.generate(**inputs, max_new_tokens=64, repetition_penalty=1.1)\n",
    "#print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "model_lora = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    'lora',\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "base_model = model_lora.merge_and_unload()\n",
    "base_model.save_pretrained('merge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6410a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#然后python merge.py,\n",
    "#./merge目录即合并后的模型, \n",
    "#然后拷贝./merge/*.bin到./base覆盖原有文件或一起都拷贝到新目录, 然后测试, 然后上传到s3桶, 最后按之前的部署方式进行部署测试."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a01965",
   "metadata": {},
   "source": [
    "## 推理测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#按之前的部署方式 baichuan2-LMI-HuggingFaceAccelerateModel.ipynb 进行部署测试.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
